
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>In Depth: Linear Regression &#8212; 《计算传播基础》</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="In-Depth: Support Vector Machines" href="09-07-support-vector-machines.html" />
    <link rel="prev" title="In Depth: Naive Bayes Classification" href="09-05-naive-bayes.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/Socratessee.gif" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">《计算传播基础》</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   寻找人类传播行为的基因
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01-intro2cjc.html">
   第一章 计算传播学简介
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-python-intro.html">
   第二章 数据科学的编程工具
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-crawler-beautifulsoup.html">
   第三章 数据抓取
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-data-cleaning-intro.html">
   第四章 数据清洗
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-01-statistics-thinking.html">
   第五章 统计思维
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="09-01-machine-learning-with-sklearn.html">
   第六章 社会科学家的机器学习
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="09-03-hyperparameters-and-model-validation.html">
     Hyperparameters and Model Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09-04-feature-engineering.html">
     Feature Engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09-05-naive-bayes.html">
     In Depth: Naive Bayes Classification
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     In Depth: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09-07-support-vector-machines.html">
     In-Depth: Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09-08-random-forests.html">
     In-Depth: Decision Trees and Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09-09-googleflustudy.html">
     Forecasting and nowcasting with Google Flu Trends
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09-10-future-employment.html">
     The future of employment
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-11-neural-network-intro.html">
   第七章 神经网络与深度学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-text-minning-gov-report.html">
   第八章 文本挖掘
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13-recsys-intro.html">
   第九章 推荐系统简介
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15-network-science-intro.html">
   第十章 网络科学简介
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="19-visualization-with-seaborn.html">
   第十一章 可视化
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/09-06-linear-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/chengjun/mybook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/chengjun/mybook/issues/new?title=Issue%20on%20page%20%2F09-06-linear-regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/chengjun/mybook/main?urlpath=tree/09-06-linear-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/chengjun/mybook/blob/main/09-06-linear-regression.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-linear-regression">
   Simple Linear Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basis-function-regression">
   Basis Function Regression 基函数回归
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#polynomial-basis-functions">
     Polynomial basis functions 多项式基函数
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-basis-functions">
     Gaussian basis functions 高斯基函数
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization">
   Regularization 正则化
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression-l-2-regularization">
     Ridge regression (
     <span class="math notranslate nohighlight">
      \(L_2\)
     </span>
     Regularization) 岭回归
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regression-l-1-regularization">
     Lasso regression (
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     regularization) 套索回归
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-predicting-bicycle-traffic">
   Example: Predicting Bicycle Traffic
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="in-depth-linear-regression">
<h1>In Depth: Linear Regression<a class="headerlink" href="#in-depth-linear-regression" title="Permalink to this headline">¶</a></h1>
<p><img alt="image.png" src="_images/author1.png" /></p>
<!--BOOK_INFORMATION-->
<p><em>This notebook contains an excerpt from the <a class="reference external" href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a class="reference external" href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>
<p><em>The text is released under the <a class="reference external" href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a class="reference external" href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a class="reference external" href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>
<p>Naive Bayes (discussed earlier in <strong>In Depth: Naive Bayes Classification</strong>) is a good starting point for classification tasks.</p>
<p><strong>Linear regression models</strong> are a good starting point for regression tasks.</p>
<ul class="simple">
<li><p>can be fit very quickly,</p></li>
<li><p>are very interpretable.</p></li>
</ul>
<p>The simplest form of a linear regression model (i.e., fitting a straight line to data)</p>
<ul class="simple">
<li><p>Extended to model more complicated data behavior.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="simple-linear-regression">
<h2>Simple Linear Regression<a class="headerlink" href="#simple-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>We will start with the most familiar linear regression, a straight-line fit to data.
A straight-line fit is a model of the form
$<span class="math notranslate nohighlight">\(
y = ax + b
\)</span><span class="math notranslate nohighlight">\(
where \)</span>a<span class="math notranslate nohighlight">\( is commonly known as the *slope*, and \)</span>b$ is commonly known as the <em>intercept</em>.</p>
<p>Consider the following data, which is scattered about a line with a slope of 2 and an intercept of -5:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate training set</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="c1"># generate test set</span>
<span class="n">xtest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">ytest</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">xtest</span> <span class="o">-</span><span class="mi">5</span>
<span class="c1"># plot training set</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_5_0.png" src="_images/09-06-linear-regression_5_0.png" />
</div>
</div>
<p>We can use Scikit-Learn’s <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> estimator to fit this data and construct the best-fit line:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># predict ypred for the test set.</span>
<span class="n">ypred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xtest</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="c1"># plot training set as scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># plot the predicted result as a strait line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_7_0.png" src="_images/09-06-linear-regression_7_0.png" />
</div>
</div>
<p>The slope and intercept of the data are contained in the model’s fit parameters, which in Scikit-Learn are always marked by a trailing underscore.
Here the relevant parameters are <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model slope:    &quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model intercept:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model slope:     2.027208810360695
Model intercept: -4.998577085553202
</pre></div>
</div>
</div>
</div>
<p>We see that the results are very close to the inputs, as we might hope.</p>
<p><strong>Model evaluation for regression</strong></p>
<ul class="simple">
<li><p>RMSE</p></li>
<li><p>R Square</p></li>
</ul>
<p>https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Root mean square error 均方根误差,亦称标准误差</span>
<span class="c1"># https://en.wikipedia.org/wiki/Root-mean-square_deviation</span>
<span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span> 
    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">rmse</span> <span class="o">=</span> <span class="n">mse</span> <span class="o">**</span> <span class="mf">0.5</span> 
    <span class="k">return</span> <span class="n">rmse</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># R square</span>
<span class="c1"># https://en.wikipedia.org/wiki/Coefficient_of_determination</span>
<span class="k">def</span> <span class="nf">R2</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span> 
    <span class="n">residuals_sum_of_squares</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">total_sum_of_squares</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">residuals_sum_of_squares</span><span class="o">/</span><span class="n">total_sum_of_squares</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RMSE: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rmse</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R2 score: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">R2</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">))</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE: 0.1584
R2 score: 0.9992
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">explained_variance_score</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RMSE: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R2 score: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Variance score: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">explained_variance_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE: 0.1584
R2 score: 0.9992
Variance score: 0.9998
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form
$<span class="math notranslate nohighlight">\(
y = a_0 + a_1 x_1 + a_2 x_2 + \cdots
\)</span><span class="math notranslate nohighlight">\(
where there are multiple \)</span>x$ values.
Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.</p>
<p><strong>Building some example data using NumPy</strong></p>
<p>The <font color = 'red'>multidimensional nature of such regressions</font> makes them more difficult to visualize</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1">#three features</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="c1"># y is constructed from three random X features</span>
</pre></div>
</div>
</div>
</div>
<p>we can use the single <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> estimator to fit lines, planes, or hyperplanes to our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5000000000000144
[ 1.5 -2.   1. ]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="basis-function-regression">
<h2>Basis Function Regression 基函数回归<a class="headerlink" href="#basis-function-regression" title="Permalink to this headline">¶</a></h2>
<p>One trick you can use to adapt linear regression to nonlinear relationships between variables</p>
<ul class="simple">
<li><p>to transform the data according to <em>basis functions</em>.</p></li>
</ul>
<p>We have seen one version of this before, in the <code class="docutils literal notranslate"><span class="pre">PolynomialRegression</span></code> pipeline used in <strong>Hyperparameters and Model Validation</strong> and <strong>Feature Engineering</strong>.</p>
<p>The idea is to take our multidimensional linear model:
$<span class="math notranslate nohighlight">\(
y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \cdots
\)</span><span class="math notranslate nohighlight">\(
and build the \)</span>x_1, x_2, x_3,<span class="math notranslate nohighlight">\( and so on, from our single-dimensional input \)</span>x$.</p>
<p>That is, we let <span class="math notranslate nohighlight">\(x_n = f_n(x)\)</span>, where <span class="math notranslate nohighlight">\(f_n()\)</span> is some function that transforms our data.</p>
<p>For example, if <span class="math notranslate nohighlight">\(f_n(x) = x^n\)</span>, our model becomes a polynomial regression:
$<span class="math notranslate nohighlight">\(
y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \cdots
\)</span>$</p>
<p>Notice that this is <em>still a linear model</em></p>
<ul class="simple">
<li><p>the linearity refers to the fact that the coefficients <span class="math notranslate nohighlight">\(a_n\)</span> never multiply or divide each other.</p></li>
<li><p>What we have effectively done is taken our one-dimensional <span class="math notranslate nohighlight">\(x\)</span> values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
<div class="section" id="polynomial-basis-functions">
<h3>Polynomial basis functions 多项式基函数<a class="headerlink" href="#polynomial-basis-functions" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>polynomial, Synonym: multinomial, 多项式</p>
</div></blockquote>
<p>This polynomial projection is useful enough that it is built into Scikit-Learn, using the <code class="docutils literal notranslate"><span class="pre">PolynomialFeatures</span></code> transformer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 2.,  4.,  8.],
       [ 3.,  9., 27.],
       [ 4., 16., 64.]])
</pre></div>
</div>
</div>
</div>
<p>We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value.</p>
<ul class="simple">
<li><p>This new, higher-dimensional data representation can then be plugged into a linear regression.</p></li>
<li><p>As we saw in <strong>Feature Engineering</strong>, the cleanest way to accomplish this is to use a pipeline.</p></li>
</ul>
<p>Let’s make a 7th-degree polynomial model in this way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="n">poly_model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">7</span><span class="p">),</span>
                           <span class="n">LinearRegression</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>With this transform in place, we can use the linear model to fit much more complicated relationships between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>For example, here is a sine wave with noise:</p>
<p>Our linear model, through the use of 7th-order polynomial basis functions, can provide an excellent fit to this non-linear data!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">poly_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">poly_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_30_0.png" src="_images/09-06-linear-regression_30_0.png" />
</div>
</div>
</div>
<div class="section" id="gaussian-basis-functions">
<h3>Gaussian basis functions 高斯基函数<a class="headerlink" href="#gaussian-basis-functions" title="Permalink to this headline">¶</a></h3>
<p>Of course, other basis functions are possible.
For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases.
The result might look something like the following figure:</p>
<a class="reference internal image-reference" href="_images/05.06-gaussian-basis.png"><img alt="_images/05.06-gaussian-basis.png" src="_images/05.06-gaussian-basis.png" style="width: 400px;" /></a>
<center>[figure source in Appendix](#Gaussian-Basis)</center>
<p><font size = '4pt'>The shaded regions in the plot are the scaled basis functions, and when added together they reproduce the smooth curve through the data.</font></p>
<p>These Gaussian basis functions are not built into Scikit-Learn,</p>
<ul class="simple">
<li><p>but we can write a custom transformer that will create them</p></li>
<li><p>Scikit-Learn transformers are implemented as Python classes;</p>
<ul>
<li><p>reading Scikit-Learn’s source is a good way to see how they can be created:</p></li>
</ul>
</li>
</ul>
<p>The simplest case of a normal distribution is known as the ‘’standard normal distribution’’.</p>
<div class="math notranslate nohighlight">
\[
f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2} } e^{ -\frac{(x-\mu)^2}{2\sigma^2} } \sim  e^{ -0.5 (\frac{x-\mu}{\sigma})^2}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>

<span class="k">class</span> <span class="nc">GaussianFeatures</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Uniformly spaced Gaussian features for one-dimensional input&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">sigma_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_factor</span> <span class="o">=</span> <span class="n">sigma_factor</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_gauss_basis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">arg</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">arg</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># create N centers spread along the data range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_factor</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gauss_basis</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu_</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">sigma_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">gauss_model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
                            <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">gauss_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">gauss_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_36_0.png" src="_images/09-06-linear-regression_36_0.png" />
</div>
</div>
<p>There is nothing magic about polynomial basis functions:</p>
<ul class="simple">
<li><p>You should have some sort of intuition about <strong>the generating process of your data</strong>;</p></li>
<li><p>If you think one basis or another might be appropriate, you can use them as well.</p></li>
</ul>
</div>
</div>
<div class="section" id="regularization">
<h2>Regularization 正则化<a class="headerlink" href="#regularization" title="Permalink to this headline">¶</a></h2>
<p>The introduction of basis functions into our linear regression makes the model much more flexible,</p>
<ul class="simple">
<li><p>but it also can very quickly lead to over-fitting (refer back to <strong>Hyperparameters and Model Validation</strong> for a discussion of this).</p></li>
</ul>
<p>For example, if we choose too many Gaussian basis functions, we end up with results that don’t look so good:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span>
                      <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_39_0.png" src="_images/09-06-linear-regression_39_0.png" />
</div>
</div>
<p>With the data projected to the 30-dimensional basis, the model has far too much flexibility and goes to extreme values between locations where it is constrained by data.</p>
<p>We can see the reason for this if we plot the coefficients of the Gaussian bases with respect to their locations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
    
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mu_</span><span class="p">,</span>
               <span class="n">model</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;basis location&#39;</span><span class="p">,</span>
              <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;coefficient&#39;</span><span class="p">,</span>
              <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_41_0.png" src="_images/09-06-linear-regression_41_0.png" />
</div>
</div>
<p>This is typical over-fitting behavior when basis functions overlap:</p>
<ul class="simple">
<li><p>the coefficients of adjacent basis functions blow up and cancel each other out.</p></li>
</ul>
<p>We know that such behavior is problematic</p>
<p>It would be nice if we could limit such spikes expliticly in the model</p>
<ul class="simple">
<li><p>by <strong>penalizing large values of the model parameters</strong>.</p></li>
</ul>
<p>Such a penalty is known as <em>regularization</em>, and comes in several forms.</p>
<div class="section" id="ridge-regression-l-2-regularization">
<h3>Ridge regression (<span class="math notranslate nohighlight">\(L_2\)</span> Regularization) 岭回归<a class="headerlink" href="#ridge-regression-l-2-regularization" title="Permalink to this headline">¶</a></h3>
<p><em>ridge regression</em> or <span class="math notranslate nohighlight">\(L_2\)</span> <em>regularization</em>, sometimes also called <em>Tikhonov regularization</em>.</p>
<ul class="simple">
<li><p>Perhaps the most common form of regularization</p></li>
</ul>
<p>This proceeds by penalizing the <strong>sum of squares</strong> (2-norms) of the model coefficients;</p>
<ul class="simple">
<li><p>The penalty on the model fit would be
$<span class="math notranslate nohighlight">\(
P = \alpha\sum_{n=1}^N \theta_n^2
\)</span>$</p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a free parameter that controls the strength of the penalty.</p>
<p>This type of penalized model is built into Scikit-Learn with the <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> estimator:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Ridge Regression&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_45_0.png" src="_images/09-06-linear-regression_45_0.png" />
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(\alpha\)</span> parameter is essentially a knob controlling the complexity of the resulting model.</p>
<ul class="simple">
<li><p>In the limit <span class="math notranslate nohighlight">\(\alpha \to 0\)</span>, we recover the standard linear regression result;</p></li>
<li><p>in the limit <span class="math notranslate nohighlight">\(\alpha \to \infty\)</span>, all model responses will be suppressed.</p></li>
</ul>
<p>One advantage of ridge regression in particular is that it can be computed very efficiently</p>
<ul class="simple">
<li><p>at hardly more computational cost than the original linear regression model.</p></li>
</ul>
</div>
<div class="section" id="lasso-regression-l-1-regularization">
<h3>Lasso regression (<span class="math notranslate nohighlight">\(L_1\)</span> regularization) 套索回归<a class="headerlink" href="#lasso-regression-l-1-regularization" title="Permalink to this headline">¶</a></h3>
<p>Lasso regression involves penalizing the <strong>sum of absolute values</strong> (1-norms) of regression coefficients:
$<span class="math notranslate nohighlight">\(
P = \alpha\sum_{n=1}^N |\theta_n|
\)</span>$
Though this is conceptually very similar to ridge regression, the results can differ surprisingly:</p>
<ul class="simple">
<li><p>for example, due to geometric reasons lasso regression tends to favor <em>sparse models</em> where possible:</p>
<ul>
<li><p>it preferentially sets model coefficients to exactly zero.</p></li>
</ul>
</li>
</ul>
<p>We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">))</span>
<span class="n">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Lasso Regression&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_48_0.png" src="_images/09-06-linear-regression_48_0.png" />
</div>
</div>
<p>With the lasso regression penalty, <strong>the majority of the coefficients are exactly zero</strong>,</p>
<ul class="simple">
<li><p>with the functional behavior being modeled by a small subset of the available basis functions.</p></li>
</ul>
<p>As with ridge regularization, the <span class="math notranslate nohighlight">\(\alpha\)</span> parameter tunes the strength of the penalty, and should be determined via, for example, cross-validation (refer back to <strong>Hyperparameters and Model Validation</strong> for a discussion of this).</p>
</div>
</div>
<div class="section" id="example-predicting-bicycle-traffic">
<h2>Example: Predicting Bicycle Traffic<a class="headerlink" href="#example-predicting-bicycle-traffic" title="Permalink to this headline">¶</a></h2>
<p>To predict the number of bicycle trips across Seattle’s Fremont Bridge based on weather, season, and other factors.</p>
<ul class="simple">
<li><p>we will join the bike data with another dataset, and</p></li>
<li><p>try to determine the extent to which weather and seasonal factors—temperature, precipitation, and daylight hours—affect the volume of bicycle traffic through this corridor.</p></li>
<li><p>the NOAA makes available their daily <a class="reference external" href="http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND">weather station data</a> (I used station ID USW00024233)</p></li>
<li><p>we can easily use Pandas to join the two data sources.</p></li>
</ul>
<p>We will perform a simple linear regression to relate weather and other information to bicycle counts, in order to estimate how a change in any one of these parameters affects the number of riders on a given day.</p>
<p>In particular, this is an example of how the tools of Scikit-Learn can be used in a statistical modeling framework, in which the parameters of the model are assumed to have interpretable meaning.</p>
<p>Let’s start by loading the two datasets, indexing by date:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/Fremont_Bridge.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">weather</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/BicycleWeather.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;DATE&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next we will compute the total daily bicycle traffic, and put this in its own dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">daily</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s1">&#39;d&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">daily</span><span class="p">[</span><span class="s1">&#39;Total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">daily</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">daily</span> <span class="o">=</span> <span class="n">daily</span><span class="p">[[</span><span class="s1">&#39;Total&#39;</span><span class="p">]]</span> <span class="c1"># remove other columns</span>
</pre></div>
</div>
</div>
</div>
<p>We saw previously that the patterns of use generally vary from day to day; let’s account for this in our data by adding binary columns that indicate the day of the week:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">days</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Mon&#39;</span><span class="p">,</span> <span class="s1">&#39;Tue&#39;</span><span class="p">,</span> <span class="s1">&#39;Wed&#39;</span><span class="p">,</span> <span class="s1">&#39;Thu&#39;</span><span class="p">,</span> <span class="s1">&#39;Fri&#39;</span><span class="p">,</span> <span class="s1">&#39;Sat&#39;</span><span class="p">,</span> <span class="s1">&#39;Sun&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">daily</span><span class="p">[</span><span class="n">days</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="n">daily</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">dayofweek</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Similarly, we might expect riders to behave differently on holidays; let’s add an indicator of this as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas.tseries.holiday</span> <span class="kn">import</span> <span class="n">USFederalHolidayCalendar</span>
<span class="n">cal</span> <span class="o">=</span> <span class="n">USFederalHolidayCalendar</span><span class="p">()</span>
<span class="n">holidays</span> <span class="o">=</span> <span class="n">cal</span><span class="o">.</span><span class="n">holidays</span><span class="p">(</span><span class="s1">&#39;2012&#39;</span><span class="p">,</span> <span class="s1">&#39;2016&#39;</span><span class="p">)</span>
<span class="n">daily</span> <span class="o">=</span> <span class="n">daily</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">holidays</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;holiday&#39;</span><span class="p">))</span>
<span class="n">daily</span><span class="p">[</span><span class="s1">&#39;holiday&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We also might suspect that the hours of daylight would affect how many people ride; let’s use the standard astronomical calculation to add this information:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="k">def</span> <span class="nf">hours_of_daylight</span><span class="p">(</span><span class="n">date</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mf">23.44</span><span class="p">,</span> <span class="n">latitude</span><span class="o">=</span><span class="mf">47.61</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the hours of daylight for the given date&quot;&quot;&quot;</span>
    <span class="n">days</span> <span class="o">=</span> <span class="p">(</span><span class="n">date</span> <span class="o">-</span> <span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">21</span><span class="p">))</span><span class="o">.</span><span class="n">days</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">latitude</span><span class="p">))</span>
         <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">days</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">365.25</span><span class="p">)))</span>
    <span class="k">return</span> <span class="mf">24.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arccos</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> <span class="o">/</span> <span class="mf">180.</span>

<span class="n">daily</span><span class="p">[</span><span class="s1">&#39;daylight_hrs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">hours_of_daylight</span><span class="p">,</span> <span class="n">daily</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
<span class="n">daily</span><span class="p">[[</span><span class="s1">&#39;daylight_hrs&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(8, 17)
</pre></div>
</div>
<img alt="_images/09-06-linear-regression_62_1.png" src="_images/09-06-linear-regression_62_1.png" />
</div>
</div>
<p>We can also add the average temperature and total precipitation to the data.
In addition to the inches of precipitation, let’s add a flag that indicates whether a day is dry (has zero precipitation):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># temperatures are in 1/10 deg C; convert to C</span>
<span class="n">weather</span><span class="p">[</span><span class="s1">&#39;TMIN&#39;</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">10</span>
<span class="n">weather</span><span class="p">[</span><span class="s1">&#39;TMAX&#39;</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">10</span>
<span class="n">weather</span><span class="p">[</span><span class="s1">&#39;Temp (C)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">weather</span><span class="p">[</span><span class="s1">&#39;TMIN&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">weather</span><span class="p">[</span><span class="s1">&#39;TMAX&#39;</span><span class="p">])</span>

<span class="c1"># precip is in 1/10 mm; convert to inches</span>
<span class="n">weather</span><span class="p">[</span><span class="s1">&#39;PRCP&#39;</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">254</span>
<span class="n">weather</span><span class="p">[</span><span class="s1">&#39;dry day&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">weather</span><span class="p">[</span><span class="s1">&#39;PRCP&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">daily</span> <span class="o">=</span> <span class="n">daily</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">weather</span><span class="p">[[</span><span class="s1">&#39;PRCP&#39;</span><span class="p">,</span> <span class="s1">&#39;Temp (C)&#39;</span><span class="p">,</span> <span class="s1">&#39;dry day&#39;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, let’s add a counter that increases from day 1, and measures how many years have passed.
This will let us measure any observed annual increase or decrease in daily crossings:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">daily</span><span class="p">[</span><span class="s1">&#39;annual&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">daily</span><span class="o">.</span><span class="n">index</span> <span class="o">-</span> <span class="n">daily</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">days</span> <span class="o">/</span> <span class="mf">365.</span>
</pre></div>
</div>
</div>
</div>
<p>Now our data is in order, and we can take a look at it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">daily</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Total</th>
      <th>Mon</th>
      <th>Tue</th>
      <th>Wed</th>
      <th>Thu</th>
      <th>Fri</th>
      <th>Sat</th>
      <th>Sun</th>
      <th>holiday</th>
      <th>daylight_hrs</th>
      <th>PRCP</th>
      <th>Temp (C)</th>
      <th>dry day</th>
      <th>annual</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2012-10-03</th>
      <td>3521.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.277359</td>
      <td>0.0</td>
      <td>13.35</td>
      <td>1.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2012-10-04</th>
      <td>3475.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.219142</td>
      <td>0.0</td>
      <td>13.60</td>
      <td>1.0</td>
      <td>0.002740</td>
    </tr>
    <tr>
      <th>2012-10-05</th>
      <td>3148.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.161038</td>
      <td>0.0</td>
      <td>15.30</td>
      <td>1.0</td>
      <td>0.005479</td>
    </tr>
    <tr>
      <th>2012-10-06</th>
      <td>2006.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.103056</td>
      <td>0.0</td>
      <td>15.85</td>
      <td>1.0</td>
      <td>0.008219</td>
    </tr>
    <tr>
      <th>2012-10-07</th>
      <td>2142.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>11.045208</td>
      <td>0.0</td>
      <td>15.85</td>
      <td>1.0</td>
      <td>0.010959</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>With this in place, we can choose the columns to use, and fit a linear regression model to our data.
We will set <code class="docutils literal notranslate"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>, because the daily flags essentially operate as their own day-specific intercepts:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Drop any rows with null values</span>
<span class="n">daily</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;any&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Mon&#39;</span><span class="p">,</span> <span class="s1">&#39;Tue&#39;</span><span class="p">,</span> <span class="s1">&#39;Wed&#39;</span><span class="p">,</span> <span class="s1">&#39;Thu&#39;</span><span class="p">,</span> <span class="s1">&#39;Fri&#39;</span><span class="p">,</span> <span class="s1">&#39;Sat&#39;</span><span class="p">,</span> <span class="s1">&#39;Sun&#39;</span><span class="p">,</span> <span class="s1">&#39;holiday&#39;</span><span class="p">,</span>
                <span class="s1">&#39;daylight_hrs&#39;</span><span class="p">,</span> <span class="s1">&#39;PRCP&#39;</span><span class="p">,</span> <span class="s1">&#39;dry day&#39;</span><span class="p">,</span> <span class="s1">&#39;Temp (C)&#39;</span><span class="p">,</span> <span class="s1">&#39;annual&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">daily</span><span class="p">[</span><span class="n">column_names</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">daily</span><span class="p">[</span><span class="s1">&#39;Total&#39;</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">daily</span><span class="p">[</span><span class="s1">&#39;predicted&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we can compare the total and predicted bicycle traffic visually:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">daily</span><span class="p">[[</span><span class="s1">&#39;Total&#39;</span><span class="p">,</span> <span class="s1">&#39;predicted&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-06-linear-regression_72_0.png" src="_images/09-06-linear-regression_72_0.png" />
</div>
</div>
<p>It is evident that we have missed some key features, especially during the summer time.</p>
<ul class="simple">
<li><p>Either our features are not complete</p>
<ul>
<li><p>i.e., people decide whether to ride to work based on more than just these</p></li>
</ul>
</li>
<li><p>or there are some nonlinear relationships that we have failed to take into account</p>
<ul>
<li><p>e.g., perhaps people ride less at both high and low temperatures</p></li>
</ul>
</li>
</ul>
<p>Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mon              504.882756
Tue              610.233936
Wed              592.673642
Thu              482.358115
Fri              177.980345
Sat            -1103.301710
Sun            -1133.567246
holiday        -1187.401381
daylight_hrs     128.851511
PRCP            -664.834882
dry day          547.698592
Temp (C)          65.162791
annual            26.942713
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>These numbers are difficult to interpret without some measure of their uncertainty.
We can compute these uncertainties quickly using bootstrap resamplings of the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">resample</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">*</span><span class="n">resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">coef_</span>
              <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With these errors estimated, let’s again look at the results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;effect&#39;</span><span class="p">:</span> <span class="n">params</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                    <span class="s1">&#39;error&#39;</span><span class="p">:</span> <span class="n">err</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">0</span><span class="p">)}))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              effect  error
Mon            505.0   86.0
Tue            610.0   83.0
Wed            593.0   83.0
Thu            482.0   85.0
Fri            178.0   81.0
Sat          -1103.0   80.0
Sun          -1134.0   83.0
holiday      -1187.0  163.0
daylight_hrs   129.0    9.0
PRCP          -665.0   62.0
dry day        548.0   33.0
Temp (C)        65.0    4.0
annual          27.0   18.0
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We first see that there is a relatively stable trend in the weekly baseline:</p>
<ul>
<li><p>there are many more riders on weekdays than on weekends and holidays.</p></li>
</ul>
</li>
<li><p>We see that for each additional hour of daylight, 129 ± 9 more people choose to ride;</p></li>
<li><p>a temperature increase of one degree Celsius encourages 65 ± 4 people to grab their bicycle;</p></li>
<li><p>a dry day means an average of 548 ± 33 more riders, and each inch of precipitation means 665 ± 62 more people leave their bike at home.</p></li>
</ul>
<p>Once all these effects are accounted for, we see a modest increase of 27 ± 18 new daily riders each year.</p>
<ul class="simple">
<li><p>Our model is almost certainly missing some relevant information.</p>
<ul>
<li><p>For example, nonlinear effects</p>
<ul>
<li><p>such as effects of precipitation <em>and</em> cold temperature</p></li>
</ul>
</li>
<li><p>nonlinear trends within each variable</p>
<ul>
<li><p>such as disinclination to ride at very cold and very hot temperatures</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Additionally, we have thrown away some of the finer-grained information</p>
<ul>
<li><p>such as the difference between a rainy morning and a rainy afternoon,</p></li>
</ul>
</li>
<li><p>and we have ignored correlations between days</p>
<ul>
<li><p>such as the possible effect of a rainy Tuesday on Wednesday’s numbers,</p></li>
<li><p>or the effect of an unexpected sunny day after a streak of rainy days.</p></li>
</ul>
</li>
</ul>
<p>These are all potentially interesting effects, and you now have the tools to begin exploring them if you wish!</p>
<p><img alt="image.png" src="_images/end.png" /></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="09-05-naive-bayes.html" title="previous page">In Depth: Naive Bayes Classification</a>
    <a class='right-next' id="next-link" href="09-07-support-vector-machines.html" title="next page">In-Depth: Support Vector Machines</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Cheng-Jun Wang<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>